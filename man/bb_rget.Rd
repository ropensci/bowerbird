% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rget.R
\name{bb_rget}
\alias{bb_rget}
\alias{bb_rget_default_downloads}
\title{A recursive download utility}
\usage{
bb_rget(
  url,
  level = 0,
  wait = 0,
  accept_follow = c("(/|\\\\.html?)$"),
  reject_follow = character(),
  accept_download = bb_rget_default_downloads(),
  accept_download_extra = character(),
  reject_download = character(),
  user,
  password,
  clobber = 1,
  no_parent = TRUE,
  no_parent_download = no_parent,
  no_check_certificate = FALSE,
  relative = FALSE,
  remote_time = TRUE,
  verbose = FALSE,
  show_progress = verbose,
  debug = FALSE,
  dry_run = FALSE,
  stop_on_download_error = FALSE,
  force_local_filename,
  use_url_directory = TRUE,
  no_host = FALSE,
  cut_dirs = 0L,
  link_css = "a",
  curl_opts,
  s3_args
)

bb_rget_default_downloads()
}
\arguments{
\item{url}{string: the URL to retrieve}

\item{level}{integer >=0: recursively download to this maximum depth level. Specify 0 for no recursion}

\item{wait}{numeric >=0: wait this number of seconds between successive retrievals. This option may help with servers that block users making too many requests in a short period of time}

\item{accept_follow}{character: character vector with one or more entries. Each entry specifies a regular expression that is applied to the complete URL. URLs matching all entries will be followed during the spidering process. Note that the first URL (provided via the \code{url} parameter) will always be visited, unless it matches the download criteria}

\item{reject_follow}{character: as for \code{accept_follow}, but specifying URL regular expressions to reject}

\item{accept_download}{character: character vector with one or more entries. Each entry specifies a regular expression that is applied to the complete URL. URLs that match all entries will be accepted for download. By default the \code{accept_download} parameter is that returned by \code{bb_rget_default_downloads}: use \code{bb_rget_default_downloads()} to see what that is}

\item{accept_download_extra}{character: character vector with one or more entries. If provided, URLs will be accepted for download if they match all entries in \code{accept_download} OR all entries in \code{accept_download_extra}. This is a convenient method to add one or more extra download types, without needing to re-specify the defaults in \code{accept_download}}

\item{reject_download}{character: as for \code{accept_regex}, but specifying URL regular expressions to reject}

\item{user}{string: username used to authenticate to the remote server}

\item{password}{string: password used to authenticate to the remote server}

\item{clobber}{numeric: 0=do not overwrite existing files, 1=overwrite if the remote file is newer than the local copy, 2=always overwrite existing files}

\item{no_parent}{logical: if \code{TRUE}, do not ever ascend to the parent directory when retrieving recursively. This is \code{TRUE} by default, bacause it guarantees that only the files below a certain hierarchy will be downloaded. Note that this check only applies to links on the same host as the starting \code{url}. If that URL links to files on another host, those links will be followed (unless \code{relative = TRUE})}

\item{no_parent_download}{logical: similar to \code{no_parent}, but applies only to download links. A typical use case is to set \code{no_parent} to \code{TRUE} and \code{no_parent_download} to \code{FALSE}, in which case the spidering process (following links to find downloadable files) will not ascend to the parent directory, but files can be downloaded from a directory that is not within the parent}

\item{no_check_certificate}{logical: if \code{TRUE}, don't check the server certificate against the available certificate authorities. Also don't require the URL host name to match the common name presented by the certificate. This option might be useful if trying to download files from a server with an expired certificate, but it is clearly a security risk and so should be used with caution}

\item{relative}{logical: if \code{TRUE}, only follow relative links. This can be useful for restricting what is downloaded in recursive mode}

\item{remote_time}{logical: if \code{TRUE}, attempt to set the local file's time to that of the remote file}

\item{verbose}{logical: print trace output?}

\item{show_progress}{logical: if \code{TRUE}, show download progress}

\item{debug}{logical: if \code{TRUE}, will print additional debugging information. If bb_rget is not behaving as expected, try setting this to \code{TRUE}}

\item{dry_run}{logical: if \code{TRUE}, spider the remote site and work out which files would be downloaded, but don't download them}

\item{stop_on_download_error}{logical: if \code{TRUE}, the download process will stop if any file download fails. If \code{FALSE}, the process will issue a warning and continue to the next file to download}

\item{force_local_filename}{character: if provided, then each \code{url} will be treated as a single URL (no recursion will be conducted). It will be downloaded to a file with name given \code{force_local_filename}, in a local directory determined by the \code{url}. \code{force_local_filename} should be a character vector of the same length as the \code{url} vector}

\item{use_url_directory}{logical: if \code{TRUE}, files will be saved into a local directory that follows the URL structure (e.g. files from \code{http://some.where/place} will be saved into directory \code{some.where/place}). If \code{FALSE}, files will be saved into the current directory}

\item{no_host}{logical: if \code{use_url_directory = TRUE}, specifying \code{no_host = TRUE} will remove the host name from the directory (e.g. files from files from \code{http://some.where/place} will be saved into directory \code{place})}

\item{cut_dirs}{integer: if \code{use_url_directory = TRUE}, specifying \code{cut_dirs} will remove this many directory levels from the path of the local directory where files will be saved (e.g. if \code{cut_dirs = 2}, files from \code{http://some.where/place/baa/haa} will be saved into directory \code{some.where/haa}. if \code{cut_dirs = 1} and \code{no_host = TRUE}, files from \code{http://some.where/place/baa/haa} will be saved into directory \code{baa/haa})}

\item{link_css}{string: css selector that identifies links (passed as the \code{css} parameter to \code{\link[rvest]{html_elements}}). Note that link elements must have an \code{href} attribute}

\item{curl_opts}{named list: options to use with \code{curl} downloads, passed to the \code{.list} parameter of \code{curl::new_handle}}

\item{s3_args}{list: named list or arguments to provide to \code{\link[aws.s3]{get_bucket_df}} and \code{\link[aws.s3]{put_object}}. Files will be uploaded into that bucket instead of the local filesystem}
}
\value{
a list with components 'ok' (TRUE/FALSE), 'files', and 'message' (error or other messages)
}
\description{
This function provides similar, but simplified, functionality to the the command-line \code{wget} utility. It is based on the \code{rvest} package.
}
\details{
NOTE: this is still somewhat experimental.
}
